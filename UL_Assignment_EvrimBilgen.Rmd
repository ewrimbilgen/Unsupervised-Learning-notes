---
title: "UL Assignment Papers"
output: 
   html_document:
    toc: true
    toc_depth: 5
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
author: "Evrim Bilgen"
date: "`r format(Sys.time(), '%d %B %Y')`"
description: "This assignment is about Clustering & Dimensionality reduction and market basket analysis"
---


<style>
#TOC
{
color:#00008B;
font-family: Calibri;
font-size: 16px;
border-color: #708090;
}

h1.title {
  color: #000000;
  background-color: #F5F5F5;
  opacity: 0.6;
  font-family: Calibri;
  font-size: 20px;
}
h4.author{
color:#000000;
font-family: Calibri;
background-color: #F5F5F5;
}
body {
color: #000000;
font-family: Calibri;
background-color: #F5F5F5;
}
  pre{
     color: #000000;
      background-color: #F8F8FF;
     }
</style>



# **UNSUPERVISED LEARNING** 

## **Content**{-}

This notebook contains these subjects;

* An Introduction to Machine Learning with R
* Introduction to Unsupervised Learning with Clustering
* Hierarchical Clustrering
* Dimensionality reduction &Principal component analysis (PCA)
* Market basket analysis using machine learning
     

  

---------------------------

## **An Introduction to Machine Learning with R**

### **aims&benefits**

  The project aims at providing an accessible introduction to various machine learning methods and applications in R. The core of the projects focuses on mainly unsupervised methods. 
  
  
### **about R**
  
R is one of the major languages for data science. It provides excellent visualisation features, which is essential to explore the data before submitting it to any automated learning, as well as assessing the results of the learning algorithm. Many R packages for machine learning are available off the shelf and many modern methods in statistical learning are implemented in R as part of their development.
  
###  **materials&method**

 - Observations, examples or simply data points along the rows
 - Features or variables along the columns (e.g. Iris data)
  
### **packages**

* caret
* ggplot2
* mlbench
* class
* dplyr
* VIM
* plotly
* gapminder
* dplyr
* animation
* tidyr
* RColorBrewer
* factoextra
* dendextend
* colorspace

------
   
   
## **Introduction to Unsupervised Learning with Clustering**
 
 Unsupervised learning is one of the most exciting areas of development in machine learning today. If you have explored machine learning bookwork before, you are probably familiar with the common breakout of problems in either supervised or unsupervised learning. Supervised learning encompasses the problem set of having a labeled dataset that can be used to either classify (for example, predicting smokers and non-smokers if you're looking at a lung health dataset) or fit a regression line on (for example, predicting the sale price of a home based on how many bedrooms it has). This model most closely mirrors an intuitive human approach to learning.
 
 ![*Differences between unsupervised and supervised learning*](/Users/ewrimmm/Desktop/UL Assignment/download.jpeg)
 
 So we can say this is basically dealing with machine learning problems which are unsupervised. This means the machine has access to a set of inputs, xx, but the desired outcome, yy is not available. Clearly, learning a relation between inputs and outcomes is impossible, but there are still a lot of problems of interest. In particular, we may want to find a compact representation of the inputs, be it for visualization of further processing. This is the problem of dimensionality reduction. (We can discuss it later here) For the same reasons, we may want to group similar inputs. This is the problem of clustering.
 

### **Clustering**

*"Clustering is in the eye of the beholder!"*

**What is clustering?**  where the goal is to find homogeneous subgroups within the data; the grouping is based on distance between observations.

Through clustering you can find commonalities among similar groups in your data. If you look more deeply at a cluster of similar people, you may learn that everyone in that group visits your website for longer periods of time than others. This can show you what the value is and also provides a clean sample size for future supervised learning experiments.

 For example, based on customers’ personal income, it is straightforward to divide the customers into three groups depending on arbitrarily selected values. The customers could be divided into three groups as follows:
 
 
 * Earn less than $10,000
 * Earn between $10,000 and $99,999 
 * Earn $100,000 or more
 
 In this case, the income levels were chosen somewhat subjectively based on easy-to-communicate points of delineation. However, such groupings do not indicate a natural affinity of the customers within each group. In other words, there is no inherent reason to believe that the customer making $90,000 will behave any differently than the customer making $110,000. As additional dimensions are introduced by adding more variables about the customers, the task of finding meaningful groupings becomes more complex. For instance, suppose variables such as age, years of education, household size, and annual purchase expenditures were considered along with the personal income variable. What are the natural occurring groupings of customers? This is the type of question that clustering analysis can help answer.
 

### **Identifying Clusters**






```{r, fig.align='center', fig.cap=" The figure shows two scatterplots "}
# Sample data
# Path of the file to read
#Simple Scatter Plots

plot(iris$Petal.Length, iris$Petal.Width, main="Edgar Anderson's Iris Data")

```





```{r, fig.align='center',fig.cap=" The  figure separates the scatterplots into three distinct clusters "}
library(ggplot2)
##ggplot (iris, aes (x = Sepal.Length, y = Sepal.Width, colour = Species)) + stat_density2d ()

plot(iris$Petal.Length, iris$Petal.Width, pch=21, bg=c("red","green3","blue")[unclass(iris$Species)], main="Edgar Anderson's Iris Data")
```


Simply by glancing at first figure, it should be plainly obvious where the clusters exist in our data – in real life, it will never be this easy. Now that we know that the data can be clearly separated into three clusters, now we can start to understand what differences exist between the two groups.


**Another simple example with some extra information:**

**Scattering plot with groups**

```{r}
library(ggplot2)
ggplot(mtcars, aes(x = mpg, y = drat)) +
    geom_point(aes(color = factor(gear)))
  

```



------

**Changing axis** 

```{r}
ggplot(mtcars, aes(x = log(mpg), y = log(drat))) +
    geom_point(aes(color = factor(gear)))
```


------



**Adding more information**

The reader should see the story behind the data analysis just by looking at the graph without referring additional documentation. Hence, graphs need good labels. We can add labels with labs()function.


```{r deneme}

my_graph <- ggplot(mtcars, aes(x = log(mpg), y = log(drat))) +
    geom_point(aes(color = factor(gear))) +
    stat_smooth(method = "lm",
        col = "#C42126",
        se = FALSE,
        size = 1)

my_graph +theme_dark()+
    labs(
        x = "Drat definition",
        y = "Mile per hours",
        color = "Gear",
        title = "Relation between Mile per hours and drat",
        subtitle = "Relationship break down by gear class",
        caption = "Authors own computation"
    )

```



------



### **k-means clustering**

K-means clustering is the most commonly used unsupervised machine learning algorithm for partitioning a given dataset into a set of k clusters, where k represents the number of groups pre-specified by the user. In k-means clustering, each cluster is represented by its center or **centroid** which corresponds to the mean of points assigned to the cluster. The basic idea behind k-means clustering is to define clusters and their centroids such that the total intra-cluster variation is minimized.

K-means is popular because;

* speed
* scalability

And also;

* K-means (MacQueen, 1967) is a partitional clustering algorithm

* The k-means algorithm partitions the given data into
k clusters:
    + Each cluster has a cluster center, called **centroid**.
    + k is specified by the user 
    
    

### **Basic example to understand the concept of clustering**


For simplicity, we work in two dimensions. We have data on the total spend of customers and their ages. To improve advertising, the marketing team wants to send more targeted emails to their customers. Then, in the following data, we plot the total spend and the age of the customers.


```{r, fig.align="center"}

library(ggplot2)
df <- data.frame(age = c(18, 21, 22, 24, 26, 26, 27, 30, 31, 35, 39, 40, 41, 42, 44, 46, 47, 48, 49, 54),
    spend = c(10, 11, 22, 15, 12, 13, 14, 33, 39, 37, 44, 27, 29, 20, 28, 21, 30, 31, 23, 24)
)
ggplot(df, aes(x = age, y = spend)) +
    geom_point()


```

A pattern is visible at this point

* At the bottom-left, you can see young people with a lower purchasing power
* Upper-middle reflects people with a job that they can afford spend more
* Finally, older people with a lower budget.

<center>
 ![*Differences between unsupervised and supervised learning*](/Users/ewrimmm/Desktop/UL Assignment/kmeans.png)
 </center>


Another example of clustering with using visualization:

```{r, fig.align="center",fig.cap="Figure:using Interactive graphics" }
library(ggplot2)
library(plotly)
library(gapminder)
 
p <- gapminder %>%
  filter(year==1977) %>%
  ggplot( aes(gdpPercap, lifeExp, size = pop, color=continent)) +
  geom_point() +
  scale_x_log10() +
  theme_bw()
 
ggplotly(p)
```

### **k-means algorithm**
 
 Given k, the k-means algorithm works as follows:
 
1- Specify the number of clusters (k) to be created (to be specified by users).

2- Select k data points randomly from the dataset as the initial cluster centers or means.

3- Assign each data point to their closest centroid, based on the euclidean distance between a data point and its centroid.

4- For each of the k clusters update cluster centroid by calculating the new mean values of all the data points in the cluster.

5- Iteratively minimize the total within the sum of squares: iterate steps 3 and 4 until the cluster assignments stop changing or the maximum number of iterations is reached.

The parameters that minimize the cost function are learned through an iterative process of assigning data points to clusters and then moving the clusters. A restriction for the k-means algorithm is that the dataset should be continuous.

K-means usually takes the Euclidean distance between the feature and feature :

<center>
 ![*Differences between unsupervised and supervised learning*](/Users/ewrimmm/Desktop/UL Assignment/KmeansClust8.jpg)
</center>

Different measures are available such as the Manhattan distance or Minlowski distance. Note that, K-mean returns different groups each time you run the algorithm. Recall that the first initial guesses are random and compute the distances until the algorithm reaches a homogeneity within groups. That is, k-mean is very sensitive to the first choice, and unless the number of observations and groups are small, it is almost impossible to get the same clustering.

### **Selecting the number of cluster** ###

The number of clusters depends on the nature of the data set, the industry, business and so on. However, there is a rule of thumb to select the appropriate number of clusters:

<center>
![*Differences between unsupervised and supervised learning*](/Users/ewrimmm/Desktop/UL Assignment/KmeansClust9.jpg)  
</center> <

with equals to the number of observation in the dataset.

Generally speaking, it is interesting to spend times to search for the best value of to fit with the business need.


Here is the sample data to perform our clustering analysis. This dataset contains 6259 observations and 10 features. The dataset observes the price from 1993 to 1995 of 486 personal computers in the US. The variables are price, speed, ram, screen, cd among other.

The steps that we will take

* Import data
* Train the model
* Evaluate the model



```{r computer dataset}
library(dplyr)
comp <- read.csv("/Users/ewrimmm/Desktop/UL Assignment/computers.csv")%>%
select(-c(X, cd, multi, premium))

glimpse(comp)

```

 to sum up the statistics:
 
```{r}
 
summary(comp)
 
```
 A good practice with k mean and distance calculation is to rescale the data so that the mean is equal to one and the standard deviation is equal to zero. So now, we need to rescale the variables with the scale() function of the dplyr library. The transformation reduces the impact of outliers and allows to compare a sole observation against the mean. If a standardized value is high, you can be confident that this observation is indeed above the mean (a large z-score implies that this point is far away from the mean in term of standard deviation).
 
 
```{r}
   rescale_comp_ds <- comp %>% 
    mutate(price_scal = scale(price),
    hd_scal = scale(hd),
    ram_scal = scale(ram),
    screen_scal = scale(screen),
    ads_scal = scale(ads),
    trend_scal = scale(trend)) %>%
    select(-c(price, speed, hd, ram, screen, ads, trend))
```

Now, we select the columns 2 and 3 of *rescale_comp_ds* data set and run the algorithm with k sets to 4. Plot the animation. (using with animation package)


```{r, figures-side, fig.show="hold", out.width="50%"}
set.seed(2345)
library(animation)
kmeans.ani(rescale_comp_ds[2:3], 4)

```


And, we can also interpret the animation such as:
* R randomly chooses four points
* Compute the Euclidean distance and draw the clusters. We have one cluster in green at the bottom left, one large cluster colored in black at the right and a red one between them and so on.
* Compute the centroid, i.e. the mean of the clusters
* Repeat until no data changes cluster

The algorithm converged after seven iterations. You can run the k-mean algorithm in our dataset with five clusters and call it **pc_cluster**.

```{r }
pc_cluster <-kmeans(rescale_comp_ds, 5)
```

The list pc_cluster contains seven interesting elements:

* pc_cluster$cluster: Indicates the cluster of each observation
* pc_cluster$centers: The cluster centres
* pc_cluster$totss: The total sum of squares
* pc_cluster$withinss: Within sum of square. The number of components return is equal to `k`
* pc_cluster$tot.withinss: Sum of withinss
* pc_clusterbetweenss: Total sum of square minus Within sum of square
* pc_cluster$size: Number of observation within each cluster

```{r include=FALSE}
pc_cluster_2 <-kmeans(rescale_comp_ds, 7)

center <-pc_cluster_2$centers


```


And also we can create a heat map with ggplot to help us highlight the difference between some categories.
So we need to use the **conda library** and the code to launch in the terminal:

*conda install -c r r-rcolorbrewer*

**How to create a heat map?**

* Build a data frame with the values of the center and create a variable with the number of the cluster
* Reshape the data with the gather() function of the tidyr library. You want to transform data from wide to long.
* Create the palette of colors with colorRampPalette() function


```{r}
library(tidyr)

# create dataset with the cluster number

cluster <- c(1: 7)
center_df <- data.frame(cluster, center)

# Reshape the data

center_reshape <- gather(center_df, features, values, price_scal: trend_scal)
head(center_reshape)
```

Then we can reshape the data and plot the graph and see what the clusters look like.



```{r, fig.cap="figure: a heatmap"}
 library(RColorBrewer)
# Create the palette
hm.palette <-colorRampPalette(rev(brewer.pal(10, 'RdYlGn')),space='Lab')

# Plot the heat map
ggplot(data = center_reshape, aes(x = features, y = cluster, fill = values)) +
    scale_y_continuous(breaks = seq(1, 7, by = 1)) +
    geom_tile() +
    coord_equal() +
    scale_fill_gradientn(colours = hm.palette(90)) +
    theme_classic()

```


## **Hierarchical Clustering**

**Introduction**

Both the natural and human-made world contain many examples of organizing systems into hierarchies and why, for the most part, it makes a lot of sense. A common representation that is developed from these hierarchies can be seen in tree-based data structures. Imagine that you had a parent node with any number of child nodes that could subsequently be parent nodes themselves. By organizing concepts into a tree structure, you can build an information-dense diagram that clearly shows how things are related to their peers and their larger abstract concepts.

An example from the natural world to help illustrate this concept can be seen in how we view the hierarchy of animals, which goes from parent classes to individual species:

<center>
 ![*Differences between unsupervised and supervised learning*](/Users/ewrimmm/Desktop/UL Assignment/h_cluster.jpeg)
</center>
<br><br><br><br>

Until this point, we have shown that hierarchies can be excellent structures in which to organize information that clearly show nested relationships among data points. While this is helpful in gaining an understanding of the parent/child relationships between items, it can also be very handy when forming clusters. Expanding on the animal example of the prior section, imagine that you were simply presented with two features of animals: their height (measured from the tip of the nose to the end of the tail) and their weight. Using this information, you then have to recreate the same structure in order to identify which records in your dataset correspond to dogs or cats, as well as their relative subspecies.

So, we can generally say that hierarchical clustering creates a hierarchy of clusters. It starts with all the data points assigned to clusters of their own. Then, the two nearest clusters are merged into the same cluster. In the end, the algorithm terminates when there is only one cluster left.

Following are the steps that are performed during hierarchical clustering:

1. In the beginning, every data point in the dataset is treated as a cluster which means that we have N clusters at the beginning of the algorithm for a dataset of size N.

2. The distance between all the points is calculated and two points closest to each other are merged together to form a new cluster.

3. Next, the point which is closest to the cluster formed in step 2, will be merged to the cluster.

4. Steps 2 and 3 are repeated until one large cluster is created.

5. Finally, this large cluster is divided into K small clusters with the help of dendrograms.


**Hierarchical clustering can be subdivided into two types:**

* _Agglomerative clustering_ in which, each observation is initially considered as a cluster of its own (leaf). Then, the most similar clusters are successively merged until there is just one single big cluster (root).
* _Divise clustering_, an inverse of agglomerative clustering, begins with the root, in witch all objects are included in one cluster. Then the most heterogeneous clusters are successively divided until all observation are in their own cluster



Here, we’ll use the R base USArrests data sets.
```{r}
# Load the data
data("USArrests")

# Standardize the data
df <- scale(USArrests)

# Show the first 6 rows
head(df, nrow = 6)
```

In order to decide which objects/clusters should be combined or divided, we need methods for measuring the similarity between objects.

By default, the function dist() computes the Euclidean distance between objects; however, it’s possible to indicate other metrics using the argument method.
For example, consider the R base data set USArrests, you can compute the distance matrix as follow:

```{r}
# Compute the dissimilarity matrix
# df = the standardized data
res.dist <- dist(df, method = "euclidean")

as.matrix(res.dist)[1:6, 1:6]
```
To see easily the distance information between objects, we reformat the results of the function dist() into a matrix using the as.matrix() function. In this matrix, value in the cell formed by the row i, the column j, represents the distance between object i and object j in the original data set. For instance, element 1,1 represents the distance between object 1 and itself (which is zero). Element 1,2 represents the distance between object 1 and object 2, and so on.

The R code above displays the first 6 rows and columns of the distance matrix:


### **Dendrogram**

Dendrograms correspond to the graphical representation of the hierarchical tree generated by the function hclust(). Dendrogram can be produced in R using the base function plot(res.hc), where res.hc is the output of hclust(). Here, we’ll use the function fviz_dend()[ in factoextra R package] to produce a beautiful dendrogram.


### **Linkage**
The linkage function takes the distance information, returned by the function dist(), and groups pairs of objects into clusters based on their similarity. Next, these newly formed clusters are linked to each other to create bigger clusters. This process is iterated until all the objects in the original data set are linked together in a hierarchical tree.

For example, given a distance matrix “res.dist” generated by the function dist(), the R base function hclust() can be used to create the hierarchical tree.

hclust() can be used as follow:

```{r}
res.hc <- hclust(d = res.dist, method = "ward.D2")
```

* **d:** a dissimilarity structure as produced by the dist() function.
* **method:** The agglomeration (linkage) method to be used for computing distance between clusters. Allowed values is one of “ward.D”, “ward.D2”, “single”, “complete”, “average”, “mcquitty”, “median” or “centroid”.


```{r}
# cex: label size

library("factoextra")
fviz_dend(res.hc, cex = 0.5)
```
In the dendrogram displayed above, each leaf corresponds to one object. As we move up the tree, objects that are similar to each other are combined into branches, which are themselves fused at a higher height.

The height of the fusion, provided on the vertical axis, indicates the (dis)similarity/distance between two objects/clusters. The higher the height of the fusion, the less similar the objects are. This height is known as the cophenetic distance between the two objects.

### **Verify the cluster tree**

After linking the objects in a data set into a hierarchical cluster tree, you might want to assess that the distances (i.e., heights) in the tree reflect the original distances accurately.

One way to measure how well the cluster tree generated by the hclust() function reflects your data is to compute the correlation between the cophenetic distances and the original distance data generated by the dist() function. If the clustering is valid, the linking of objects in the cluster tree should have a strong correlation with the distances between objects in the original distance matrix.

The R base function cophenetic() can be used to compute the cophenetic distances for hierarchical clustering.
```{r}
# Compute cophentic distance
res.coph <- cophenetic(res.hc)

# Correlation between cophenetic distance and
# the original distance
cor(res.dist, res.coph)
```

Execute the hclust() function again using the average linkage method. Next, call cophenetic() to evaluate the clustering solution.

```{r}
res.hc2 <- hclust(res.dist, method = "average")

cor(res.dist, cophenetic(res.hc2))
```

The correlation coefficient shows that using a different linkage method creates a tree that represents the original distances slightly better.

### **Cut the dendrogram into different groups**

One of the problems with hierarchical clustering is that, it does not tell us how many clusters there are, or where to cut the dendrogram to form clusters.

You can cut the hierarchical tree at a given height in order to partition your data into clusters. The R base function cutree() can be used to cut a tree, generated by the hclust() function, into several groups either by specifying the desired number of groups or the cut height. It returns a vector containing the cluster number of each observation

```{r}

# Cut tree into 4 groups
grp <- cutree(res.hc, k = 4)
head(grp, n = 4)

# Number of members in each cluster
table(grp)

# Get the names for the members of cluster 1
rownames(df)[grp == 1]



```

The result of the cuts can be visualized easily using the function  [in factoextra]:


```{r}
 fviz_dend(res.hc, cex = 0.5, k = 4, color_labels_by_k = TRUE)
```


To understand better, Let's take a look in another example. But first, we need to know the data that we will use.

The Iris flower data set or Fisher's Iris data set is a multivariate data set introduced by the British statistician, eugenicist, and biologist Ronald Fisher in his 1936 paper The use of multiple measurements in taxonomic problems as an example of linear discriminant analysis.[1] It is sometimes called Anderson's Iris data set because Edgar Anderson collected the data to quantify the morphologic variation of Iris flowers of three related species
The use of this data set in cluster analysis however is not common, since the data set only contains two clusters with rather obvious separation. One of the clusters contains Iris setosa, while the other cluster contains both Iris virginica and Iris versicolor and is not separable without the species information Fisher used. This makes the data set a good example to explain the difference between supervised and unsupervised techniques in data mining.



```{r}

iris <- datasets::iris
iris2 <- iris[,-5]
species_labels <- iris[,5]
library(colorspace) # get nice colors
species_col <- rev(rainbow_hcl(3))[as.numeric(species_labels)]
d_iris <- dist(iris2) # method="man" # is a bit better
hc_iris <- hclust(d_iris, method = "complete")
iris_species <- rev(levels(iris[,5]))

library(dendextend)
dend <- as.dendrogram(hc_iris)
# order it the closest we can to the order of the observations:
dend <- rotate(dend, 1:150)

# Color the branches based on the clusters:
dend <- color_branches(dend, k=3) #, groupLabels=iris_species)

# Manually match the labels, as much as possible, to the real classification of the flowers:
labels_colors(dend) <-
   rainbow_hcl(3)[sort_levels_values(
      as.numeric(iris[,5])[order.dendrogram(dend)]
   )]

# We shall add the flower type to the labels:
labels(dend) <- paste(as.character(iris[,5])[order.dendrogram(dend)],
                           "(",labels(dend),")", 
                           sep = "")
# We hang the dendrogram a bit:
dend <- hang.dendrogram(dend,hang_height=0.1)
# reduce the size of the labels:
# dend <- assign_values_to_leaves_nodePar(dend, 0.5, "lab.cex")
dend <- set(dend, "labels_cex", 0.5)
# And plot:
par(mar = c(3,3,3,7))
plot(dend, 
     main = "Clustered Iris data set
     (the labels give the true flower species)", 
     horiz =  TRUE,  nodePar = list(cex = .007))
legend("topleft", legend = iris_species, fill = rainbow_hcl(3))

```

The default hierarchical clustering method in hclust is **complete**. We can visualize the result of running it by turning the object to a dendrogram and making several adjustments to the object, such as: changing the labels, coloring the labels based on the real species category, and coloring the branches based on cutting the tree into three clusters.


**Key points of this visualisation:**

* These visualizations easily demonstrates how the separation of the hierarchical clustering is very good with the “Setosa” species, but misses in labeling many “Versicolor” species as “Virginica”.

* The hanging of the tree also helps to locate extreme observations. For example, we can see that observation “virginica (107)” is not very similar to the Versicolor species, but still, it is among them. Also, “Versicolor (71)” is located too much “within” the group of Virginica flowers.


**Two important things that you should know about hierarchical clustering are:**

* This algorithm has been implemented above using bottom up approach. It is also possible to follow top-down approach starting with all data points assigned in the same cluster and recursively performing splits till each data point is assigned a separate cluster.
* The decision of merging two clusters is taken on the basis of closeness of these clusters. There are multiple metrics for deciding the closeness of two clusters :
    + Euclidean distance: ||a-b||2 = √(Σ(ai-bi))
    + Squared Euclidean distance: ||a-b||22 = Σ((ai-bi)2)
    + Manhattan distance: ||a-b||1 = Σ|ai-bi|
    + Maximum distance:||a-b||INFINITY = maxi|ai-bi|
    + Mahalanobis distance: √((a-b)T S-1 (-b))   {where, s : covariance matrix}
    
    
    
### **Difference between K Means and Hierarchical clustering**    
    
  * Hierarchical clustering can’t handle big data well but K Means clustering can. This is because the time complexity of K Means is linear and hierarchical clustering is quadratic.
  * In K Means clustering, since we start with random choice of clusters, the results produced by running the algorithm multiple times might differ. While results are reproducible in Hierarchical clustering.
  * K Means is found to work well when the shape of the clusters is hyper spherical (like circle in 2D, sphere in 3D).
  * K Means clustering requires prior knowledge of K i.e. no. of clusters we want to divide our data into. But, we can stop at whatever number of clusters we find appropriate in hierarchical clustering by interpreting the dendrogram
    
    
### **Applications of Clustering**    

Clustering has popular across various domains. Some of the most popular applications of clustering are:

  * Recommendation engines
  * Market segmentation
  * Social network analysis
  * Search result grouping
  * Medical imaging
  * Image segmentation
  * Anomaly detection


    
